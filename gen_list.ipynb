{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "#Download the source data from here: https://dataturks.com/projects/jpark23/Audit%20report/overview?type=all\n",
    "json_data = \"F:\\\\WB\\\\nlpcode\\\\sampledocs\\\\Audit\\\\Audit.json\"\n",
    "with open(json_data, encoding=\"utf8\") as data_file:\n",
    "        data_item = json.load(data_file)\n",
    "data = data_item['annotation']\n",
    "\n",
    "del data[-4]#drop duplicates\n",
    "del data[-3]#drop duplicates\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Capacity Building', 'Program Guidelines', 'Arrangements', 'Key Mission', 'Other']\n"
     ]
    }
   ],
   "source": [
    "#Create a column list\n",
    "col_list = [item['label'][0] for item in data]\n",
    "col_list = col_list[1:]\n",
    "print(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other 158\n",
      "Capacity Building 6\n",
      "Program Guidelines 11\n",
      "Arrangements 16\n",
      "Key Mission 5\n",
      "Other 38\n"
     ]
    }
   ],
   "source": [
    "#Show the number of sentences in each category (label)\n",
    "from nltk import tokenize\n",
    "for i in range(len(data)):\n",
    "    sentences = tokenize.sent_tokenize(data[i]['points'][0]['text'])\n",
    "    print(data[i]['label'][0] + \" \" + str(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "# Generate a new list including mix of original and random sentences for each label\n",
    "import random\n",
    "import math\n",
    "from nltk import tokenize\n",
    "\n",
    "\n",
    "others = tokenize.sent_tokenize(data[0]['points'][0]['text'])\n",
    "gen_lst = []\n",
    "for k in range(100):\n",
    "    for i in range(1,len(data)):\n",
    "        sentences = tokenize.sent_tokenize(data[i]['points'][0]['text'])\n",
    "        rnd = random.sample(range(len(sentences)), math.ceil(len(sentences)/10))\n",
    "        org_sentences = [sentences[i] for i in rnd]\n",
    "        rnd = random.sample(range(len(others)), math.ceil(9 * len(sentences)/10))\n",
    "        chn_sentences = [others[i] for i in rnd]\n",
    "        new_sentences = org_sentences + chn_sentences\n",
    "        gen_lst.append([col_list[i-1],''.join(new_sentences)])\n",
    "   \n",
    "\n",
    "print(len(gen_lst))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other 3170\n",
      "Capacity Building 162\n",
      "Program Guidelines 194\n",
      "Arrangements 276\n",
      "Key Mission 77\n",
      "Other 1000\n"
     ]
    }
   ],
   "source": [
    "#Shows the number of words for each label\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set (stopwords.words( 'english' ))\n",
    "for i in range(len(data)):\n",
    "    words  = nltk.word_tokenize(data[i]['points'][0]['text'])\n",
    "    filtered_words = [w for w in words if not w in stop_words]\n",
    "    print(data[i]['label'][0] + \" \" + str(len(filtered_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4879\n"
     ]
    }
   ],
   "source": [
    "#Generate the list of all words after removing stop words\n",
    "stop_words = set (stopwords.words( 'english' ))\n",
    "\n",
    "all_words = []\n",
    "for i in range(len(data)):\n",
    "    all_words += nltk.word_tokenize(data[i]['points'][0]['text'])\n",
    "\n",
    "filtered_all_words = [w for w in all_words if not w in stop_words]\n",
    "\n",
    "print(len(filtered_all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new list including mix of original and random words for each label\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "gen_lst = []\n",
    "for k in range(100):\n",
    "    for i in range(1,len(data)):\n",
    "        words = nltk.word_tokenize(data[i]['points'][0]['text'])\n",
    "        filtered_words = [w for w in words if not w in stop_words]\n",
    "        filtered_others = [w for w in filtered_all_words if not w in filtered_words]\n",
    " \n",
    "        rnd = random.sample(range(len(filtered_words)), math.ceil(len(filtered_words)/10))\n",
    "        org_words = [filtered_words[i] for i in rnd]\n",
    "        \n",
    "        rnd = random.sample(range(len(filtered_others)), math.ceil(9 * len(filtered_words)/10))\n",
    "        chn_words = [filtered_others[i] for i in rnd]\n",
    "        \n",
    "        new_words = org_words + chn_words\n",
    "        gen_lst.append([col_list[i-1],' '.join(new_words)])\n",
    "        \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pharagraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Capacity Building</td>\n",
       "      <td>’ Planning arrangements February preparation :...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Program Guidelines</td>\n",
       "      <td>jointly assessment . ’ field Sanitation water ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arrangements</td>\n",
       "      <td>carry Bank perception strongly Rural team Bank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Key Mission</td>\n",
       "      <td>implementation : 1 taken implementation fully ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Other</td>\n",
       "      <td>instructions Participating strengthen , ) PPCs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                label                                         pharagraph\n",
       "0   Capacity Building  ’ Planning arrangements February preparation :...\n",
       "1  Program Guidelines  jointly assessment . ’ field Sanitation water ...\n",
       "2        Arrangements  carry Bank perception strongly Rural team Bank...\n",
       "3         Key Mission  implementation : 1 taken implementation fully ...\n",
       "4               Other  instructions Participating strengthen , ) PPCs..."
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert the list to dataframe \n",
    "import pandas as pd\n",
    "df = pd.DataFrame(gen_lst, columns = ['label','pharagraph'])\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
